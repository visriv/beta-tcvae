{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5266d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd beta-tcvae\n",
    "# %cd content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3104d725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'lib.utils' from '/Users/visriv/Documents/Git/beta-tcvae/lib/utils.py'>"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "from numbers import Number\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets\n",
    "import visdom\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import brewer2mpl\n",
    "from tqdm import tqdm\n",
    "\n",
    "import lib.dist as dist\n",
    "import lib.utils as utils\n",
    "import lib.datasets as dset\n",
    "from lib.flows import FactorialNormalizingFlow\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "# from metric_helpers.loader import load_model_and_dataset\n",
    "\n",
    "from plot_latent_vs_true import plot_vs_gt_shapes, plot_vs_gt_faces  # noqa: F401\n",
    "from lib.model import *\n",
    "from metric_helpers.elbo_decomposition import *\n",
    "from metric_helpers.mi_metric import compute_metric_shapes, compute_metric_faces, compute_metric_mnist\n",
    "\n",
    "\n",
    "import metric_helpers\n",
    "reload(metric_helpers.elbo_decomposition)\n",
    "reload(utils)\n",
    "# reload(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6cdfc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_samples = None\n",
    "win_test_reco = None\n",
    "win_latent_walk = None\n",
    "win_train_elbo = None\n",
    "\n",
    "\n",
    "\n",
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "args = Object()\n",
    "args.dataset = 'mnist'\n",
    "args.dist = 'normal'\n",
    "args.num_epochs = 10\n",
    "args.batch_size = 32\n",
    "args.learning_rate = 1e-3\n",
    "args.latent_dim = 10\n",
    "args.beta = 1\n",
    "args.tcvae = True\n",
    "args.exclude_mutinfo = False\n",
    "args.beta_anneal = False\n",
    "args.lambda_anneal = False\n",
    "args.mss = False\n",
    "args.conv = True\n",
    "args.gpu = 0\n",
    "args.visdom = False\n",
    "args.save = 'test'\n",
    "args.log_freq = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d497662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.set_device(args.gpu)\n",
    "use_cuda = False\n",
    "# data loader\n",
    "train_loader = utils.setup_data_loaders(args, use_cuda=False)\n",
    "\n",
    "\n",
    "\n",
    "# setup the VAE\n",
    "if args.dist == 'normal':\n",
    "    prior_dist = dist.Normal()\n",
    "    q_dist = dist.Normal()\n",
    "elif args.dist == 'laplace':\n",
    "    prior_dist = dist.Laplace()\n",
    "    q_dist = dist.Laplace()\n",
    "elif args.dist == 'flow':\n",
    "    prior_dist = FactorialNormalizingFlow(dim=args.latent_dim, nsteps=32)\n",
    "    q_dist = dist.Normal()\n",
    "\n",
    "vae = VAE(z_dim=args.latent_dim, use_cuda=use_cuda, prior_dist=prior_dist, q_dist=q_dist, image_dim = 28,\n",
    "    include_mutinfo=not args.exclude_mutinfo, tcvae=args.tcvae, conv=args.conv, mss=args.mss)\n",
    "\n",
    "# setup the optimizer\n",
    "optimizer = optim.Adam(vae.parameters(), lr=args.learning_rate)\n",
    "\n",
    "# setup visdom for visualization\n",
    "if args.visdom:\n",
    "    vis = visdom.Visdom(env=args.save, port=4500)\n",
    "\n",
    "train_elbo = []\n",
    "\n",
    "# training loop\n",
    "dataset_size = len(train_loader.dataset)\n",
    "num_iterations = len(train_loader) * args.num_epochs\n",
    "iteration = 0\n",
    "# initialize loss accumulator\n",
    "elbo_running_mean = utils.RunningAverageMeter()\n",
    "while iteration < num_iterations:\n",
    "    for i, x in enumerate(train_loader):\n",
    "        iteration += 1\n",
    "        print(iteration)\n",
    "        batch_time = time.time()\n",
    "        vae.train()\n",
    "        anneal_kl(args, vae, iteration)\n",
    "        optimizer.zero_grad()\n",
    "        # transfer to GPU\n",
    "        x = x[0]\n",
    "        # print(type(x))\n",
    "        # print(x)\n",
    "        # print(len(x), len(x[0]), len(x[0][0]), len(x[0][0][0]))\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        # wrap the mini-batch in a PyTorch Variable\n",
    "        x = Variable(x)\n",
    "        # do ELBO gradient and accumulate loss\n",
    "        obj, elbo = vae.elbo(x, dataset_size, 28)\n",
    "#         print('elbo', elbo)\n",
    "        if utils.isnan(obj).any():\n",
    "            raise ValueError('NaN spotted in objective.')\n",
    "        obj.mean().mul(-1).backward()\n",
    "        elbo_running_mean.update(torch.mean(elbo.mean()))\n",
    "        optimizer.step()\n",
    "\n",
    "        # report training diagnostics\n",
    "        if iteration % args.log_freq == 0:\n",
    "            train_elbo.append(elbo_running_mean.avg)\n",
    "            print('[iteration %03d] time: %.2f \\tbeta %.2f \\tlambda %.2f training ELBO: %.4f (%.4f)' % (\n",
    "                iteration, time.time() - batch_time, vae.beta, vae.lamb,\n",
    "                elbo_running_mean.val, elbo_running_mean.avg))\n",
    "\n",
    "            vae.eval()\n",
    "\n",
    "            # plot training and test ELBOs\n",
    "            if args.visdom:\n",
    "                display_samples(vae, x, vis)\n",
    "                plot_elbo(train_elbo, vis)\n",
    "\n",
    "            utils.save_checkpoint({\n",
    "                'state_dict': vae.state_dict(),\n",
    "                'args': args}, args.save, 0)\n",
    "            eval('plot_vs_gt_' + args.dataset)(vae, train_loader.dataset,\n",
    "                                               os.path.join(args.save, 'gt_vs_latent_{:05d}.png'.format(iteration)),\n",
    "                                               use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f81f2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.set_device(args.gpu)\n",
    "use_cuda = False\n",
    "# data loader\n",
    "train_loader = utils.setup_data_loaders(args, use_cuda=False)\n",
    "\n",
    "\n",
    "\n",
    "# setup the VAE\n",
    "if args.dist == 'normal':\n",
    "    prior_dist = dist.Normal()\n",
    "    q_dist = dist.Normal()\n",
    "elif args.dist == 'laplace':\n",
    "    prior_dist = dist.Laplace()\n",
    "    q_dist = dist.Laplace()\n",
    "elif args.dist == 'flow':\n",
    "    prior_dist = FactorialNormalizingFlow(dim=args.latent_dim, nsteps=32)\n",
    "    q_dist = dist.Normal()\n",
    "\n",
    "vae = VAE(z_dim=args.latent_dim, use_cuda=use_cuda, prior_dist=prior_dist, q_dist=q_dist, image_dim = 28,\n",
    "    include_mutinfo=not args.exclude_mutinfo, tcvae=args.tcvae, conv=args.conv, mss=args.mss)\n",
    "\n",
    "# setup the optimizer\n",
    "optimizer = optim.Adam(vae.parameters(), lr=args.learning_rate)\n",
    "\n",
    "# setup visdom for visualization\n",
    "if args.visdom:\n",
    "    vis = visdom.Visdom(env=args.save, port=4500)\n",
    "\n",
    "train_elbo = []\n",
    "\n",
    "# training loop\n",
    "dataset_size = len(train_loader.dataset)\n",
    "num_iterations = len(train_loader) * args.num_epochs\n",
    "iteration = 0\n",
    "# initialize loss accumulator\n",
    "elbo_running_mean = utils.RunningAverageMeter()\n",
    "while iteration < num_iterations:\n",
    "    for i, x in enumerate(train_loader):\n",
    "        iteration += 1\n",
    "        print(iteration)\n",
    "        batch_time = time.time()\n",
    "        vae.train()\n",
    "        utils.anneal_kl(args, vae, iteration)\n",
    "        optimizer.zero_grad()\n",
    "        # transfer to GPU\n",
    "        x = x[0]\n",
    "        # print(type(x))\n",
    "        # print(x)\n",
    "        # print(len(x), len(x[0]), len(x[0][0]), len(x[0][0][0]))\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        # wrap the mini-batch in a PyTorch Variable\n",
    "        x = Variable(x)\n",
    "        # do ELBO gradient and accumulate loss\n",
    "        obj, elbo = vae.elbo(x, dataset_size, 28)\n",
    "#         print('elbo', elbo)\n",
    "        if utils.isnan(obj).any():\n",
    "            raise ValueError('NaN spotted in objective.')\n",
    "        obj.mean().mul(-1).backward()\n",
    "        elbo_running_mean.update(torch.mean(elbo.mean()))\n",
    "        optimizer.step()\n",
    "\n",
    "        # report training diagnostics\n",
    "        if iteration % args.log_freq == 0:\n",
    "            train_elbo.append(elbo_running_mean.avg)\n",
    "            print('[iteration %03d] time: %.2f \\tbeta %.2f \\tlambda %.2f training ELBO: %.4f (%.4f)' % (\n",
    "                iteration, time.time() - batch_time, vae.beta, vae.lamb,\n",
    "                elbo_running_mean.val, elbo_running_mean.avg))\n",
    "\n",
    "            vae.eval()\n",
    "\n",
    "            # plot training and test ELBOs\n",
    "            if args.visdom:\n",
    "                display_samples(vae, x, vis)\n",
    "                plot_elbo(train_elbo, vis)\n",
    "\n",
    "            utils.save_checkpoint({\n",
    "                'state_dict': vae.state_dict(),\n",
    "                'args': args}, args.save, 0)\n",
    "            eval('plot_vs_gt_' + args.dataset)(vae, train_loader.dataset,\n",
    "                                               os.path.join(args.save, 'gt_vs_latent_{:05d}.png'.format(iteration)),\n",
    "                                               use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c9a69a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'lib.utils' from '/Users/visriv/Documents/Git/beta-tcvae/lib/utils.py'>"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "from numbers import Number\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets\n",
    "import visdom\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import brewer2mpl\n",
    "from tqdm import tqdm\n",
    "\n",
    "import lib.dist as dist\n",
    "import lib.utils as utils\n",
    "import lib.datasets as dset\n",
    "from lib.flows import FactorialNormalizingFlow\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "# from metric_helpers.loader import load_model_and_dataset\n",
    "\n",
    "from plot_latent_vs_true import plot_vs_gt_shapes, plot_vs_gt_faces, plot_vs_gt_mnist # noqa: F401\n",
    "from lib.model import *\n",
    "from metric_helpers.elbo_decomposition import *\n",
    "from metric_helpers.mi_metric import compute_metric_shapes, compute_metric_faces, compute_metric_mnist\n",
    "\n",
    "\n",
    "import metric_helpers\n",
    "reload(metric_helpers.elbo_decomposition)\n",
    "reload(utils)\n",
    "# reload(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0782d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.set_device(args.gpu)\n",
    "use_cuda = False\n",
    "# data loader\n",
    "train_loader = utils.setup_data_loaders(args, use_cuda=False)\n",
    "\n",
    "\n",
    "\n",
    "# setup the VAE\n",
    "if args.dist == 'normal':\n",
    "    prior_dist = dist.Normal()\n",
    "    q_dist = dist.Normal()\n",
    "elif args.dist == 'laplace':\n",
    "    prior_dist = dist.Laplace()\n",
    "    q_dist = dist.Laplace()\n",
    "elif args.dist == 'flow':\n",
    "    prior_dist = FactorialNormalizingFlow(dim=args.latent_dim, nsteps=32)\n",
    "    q_dist = dist.Normal()\n",
    "\n",
    "vae = VAE(z_dim=args.latent_dim, use_cuda=use_cuda, prior_dist=prior_dist, q_dist=q_dist, image_dim = 28,\n",
    "    include_mutinfo=not args.exclude_mutinfo, tcvae=args.tcvae, conv=args.conv, mss=args.mss)\n",
    "\n",
    "# setup the optimizer\n",
    "optimizer = optim.Adam(vae.parameters(), lr=args.learning_rate)\n",
    "\n",
    "# setup visdom for visualization\n",
    "if args.visdom:\n",
    "    vis = visdom.Visdom(env=args.save, port=4500)\n",
    "\n",
    "train_elbo = []\n",
    "\n",
    "# training loop\n",
    "dataset_size = len(train_loader.dataset)\n",
    "num_iterations = len(train_loader) * args.num_epochs\n",
    "iteration = 0\n",
    "# initialize loss accumulator\n",
    "elbo_running_mean = utils.RunningAverageMeter()\n",
    "while iteration < num_iterations:\n",
    "    for i, x in enumerate(train_loader):\n",
    "        iteration += 1\n",
    "        print(iteration)\n",
    "        batch_time = time.time()\n",
    "        vae.train()\n",
    "        utils.anneal_kl(args, vae, iteration)\n",
    "        optimizer.zero_grad()\n",
    "        # transfer to GPU\n",
    "        x = x[0]\n",
    "        # print(type(x))\n",
    "        # print(x)\n",
    "        # print(len(x), len(x[0]), len(x[0][0]), len(x[0][0][0]))\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        # wrap the mini-batch in a PyTorch Variable\n",
    "        x = Variable(x)\n",
    "        # do ELBO gradient and accumulate loss\n",
    "        obj, elbo = vae.elbo(x, dataset_size, 28)\n",
    "#         print('elbo', elbo)\n",
    "        if utils.isnan(obj).any():\n",
    "            raise ValueError('NaN spotted in objective.')\n",
    "        obj.mean().mul(-1).backward()\n",
    "        elbo_running_mean.update(torch.mean(elbo.mean()))\n",
    "        optimizer.step()\n",
    "\n",
    "        # report training diagnostics\n",
    "        if iteration % args.log_freq == 0:\n",
    "            train_elbo.append(elbo_running_mean.avg)\n",
    "            print('[iteration %03d] time: %.2f \\tbeta %.2f \\tlambda %.2f training ELBO: %.4f (%.4f)' % (\n",
    "                iteration, time.time() - batch_time, vae.beta, vae.lamb,\n",
    "                elbo_running_mean.val, elbo_running_mean.avg))\n",
    "\n",
    "            vae.eval()\n",
    "\n",
    "            # plot training and test ELBOs\n",
    "            if args.visdom:\n",
    "                display_samples(vae, x, vis)\n",
    "                plot_elbo(train_elbo, vis)\n",
    "\n",
    "            utils.save_checkpoint({\n",
    "                'state_dict': vae.state_dict(),\n",
    "                'args': args}, args.save, 0)\n",
    "            eval('plot_vs_gt_' + args.dataset)(vae, train_loader.dataset,\n",
    "                                               os.path.join(args.save, 'gt_vs_latent_{:05d}.png'.format(iteration)),\n",
    "                                               use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7a5456f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47f3e26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<wandb.sdk.wandb_run.Run at 0x13bf2b8b0>"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/visriv/Documents/Git/beta-tcvae/wandb/run-20230320_145718-y37n00go</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/visriv/my-awesome-project/runs/y37n00go' target=\"_blank\">giddy-water-1</a></strong> to <a href='https://wandb.ai/visriv/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/visriv/my-awesome-project' target=\"_blank\">https://wandb.ai/visriv/my-awesome-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/visriv/my-awesome-project/runs/y37n00go' target=\"_blank\">https://wandb.ai/visriv/my-awesome-project/runs/y37n00go</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"my-awesome-project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "170f7566",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"my-awesome-project\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
