diff --git a/disentanglement_metrics.py b/disentanglement_metrics.py
deleted file mode 100644
index 7fe6270..0000000
--- a/disentanglement_metrics.py
+++ /dev/null
@@ -1,314 +0,0 @@
-import math
-import os
-import torch
-from tqdm import tqdm
-from torch.utils.data import DataLoader
-from torch.autograd import Variable
-
-import lib.utils as utils
-from metric_helpers.loader import load_model_and_dataset
-from metric_helpers.mi_metric import compute_metric_shapes, compute_metric_faces
-
-def mutual_info_metric_mnist(vae, shapes_dataset):
-    dataset_loader = DataLoader(shapes_dataset, batch_size=1000, num_workers=1, shuffle=False)
-
-    N = len(dataset_loader.dataset)  # number of data samples
-    K = vae.z_dim                    # number of latent variables
-    nparams = vae.q_dist.nparams
-    vae.eval()
-
-    print('Computing q(z|x) distributions.')
-    qz_params = torch.Tensor(N, K, nparams)
-
-    n = 0
-    for xs in dataset_loader:
-        batch_size = xs.size(0)
-        xs = Variable(xs.view(batch_size, 1, 28, 28).cuda(), volatile=True)
-        qz_params[n:n + batch_size] = vae.encoder.forward(xs).view(batch_size, vae.z_dim, nparams).data
-        n += batch_size
-
-    qz_params = Variable(qz_params.view(3, 10, 5, 20, 20, K, nparams).cuda())
-    qz_samples = vae.q_dist.sample(params=qz_params)
-
-    print('Estimating marginal entropies.')
-    # marginal entropies
-    marginal_entropies = estimate_entropies(
-        qz_samples.view(N, K).transpose(0, 1),
-        qz_params.view(N, K, nparams),
-        vae.q_dist)
-
-    marginal_entropies = marginal_entropies.cpu()
-    cond_entropies = torch.zeros(3, K)
-
-    print('Estimating conditional entropies for azimuth.')
-    for i in range(10):
-        qz_samples_pose_az = qz_samples[:, i, :, :, :].contiguous()
-        qz_params_pose_az = qz_params[:, i, :, :, :].contiguous()
-
-        cond_entropies_i = estimate_entropies(
-            qz_samples_pose_az.view(N // 5, K).transpose(0, 1),
-            qz_params_pose_az.view(N // 5, K, nparams),
-            vae.q_dist)
-
-        cond_entropies[0] += cond_entropies_i.cpu() / 5
-
-    print('Estimating conditional entropies for elevation.')
-    for i in range(20):
-        qz_samples_pose_el = qz_samples[:, :, i, :, :].contiguous()
-        qz_params_pose_el = qz_params[:, :, i, :, :].contiguous()
-
-        cond_entropies_i = estimate_entropies(
-            qz_samples_pose_el.view(N // 20, K).transpose(0, 1),
-            qz_params_pose_el.view(N // 20, K, nparams),
-            vae.q_dist)
-
-        cond_entropies[1] += cond_entropies_i.cpu() / 20
-
-    print('Estimating conditional entropies for lighting.')
-    for i in range(20):
-        qz_samples_lighting = qz_samples[:, :, :, i, :].contiguous()
-        qz_params_lighting = qz_params[:, :, :, i, :].contiguous()
-
-        cond_entropies_i = estimate_entropies(
-            qz_samples_lighting.view(N // 20, K).transpose(0, 1),
-            qz_params_lighting.view(N // 20, K, nparams),
-            vae.q_dist)
-
-        cond_entropies[2] += cond_entropies_i.cpu() / 20
-
-    metric = compute_metric_faces(marginal_entropies, cond_entropies)
-    return metric, marginal_entropies, cond_entropies
-
-
-
-
-
-def estimate_entropies(qz_samples, qz_params, q_dist, n_samples=10000, weights=None):
-    """Computes the term:
-        E_{p(x)} E_{q(z|x)} [-log q(z)]
-    and
-        E_{p(x)} E_{q(z_j|x)} [-log q(z_j)]
-    where q(z) = 1/N sum_n=1^N q(z|x_n).
-    Assumes samples are from q(z|x) for *all* x in the dataset.
-    Assumes that q(z|x) is factorial ie. q(z|x) = prod_j q(z_j|x).
-
-    Computes numerically stable NLL:
-        - log q(z) = log N - logsumexp_n=1^N log q(z|x_n)
-
-    Inputs:
-    -------
-        qz_samples (K, N) Variable
-        qz_params  (N, K, nparams) Variable
-        weights (N) Variable
-    """
-
-    # Only take a sample subset of the samples
-    if weights is None:
-        qz_samples = qz_samples.index_select(1, Variable(torch.randperm(qz_samples.size(1))[:n_samples].cuda()))
-    else:
-        sample_inds = torch.multinomial(weights, n_samples, replacement=True)
-        qz_samples = qz_samples.index_select(1, sample_inds)
-
-    K, S = qz_samples.size()
-    N, _, nparams = qz_params.size()
-    assert(nparams == q_dist.nparams)
-    assert(K == qz_params.size(1))
-
-    if weights is None:
-        weights = -math.log(N)
-    else:
-        weights = torch.log(weights.view(N, 1, 1) / weights.sum())
-
-    entropies = torch.zeros(K).cuda()
-
-    pbar = tqdm(total=S)
-    k = 0
-    while k < S:
-        batch_size = min(10, S - k)
-        logqz_i = q_dist.log_density(
-            qz_samples.view(1, K, S).expand(N, K, S)[:, :, k:k + batch_size],
-            qz_params.view(N, K, 1, nparams).expand(N, K, S, nparams)[:, :, k:k + batch_size])
-        k += batch_size
-
-        # computes - log q(z_i) summed over minibatch
-        entropies += - utils.logsumexp(logqz_i + weights, dim=0, keepdim=False).data.sum(1)
-        pbar.update(batch_size)
-    pbar.close()
-
-    entropies /= S
-
-    return entropies
-
-
-def mutual_info_metric_shapes(vae, shapes_dataset):
-    dataset_loader = DataLoader(shapes_dataset, batch_size=1000, num_workers=1, shuffle=False)
-
-    N = len(dataset_loader.dataset)  # number of data samples
-    K = vae.z_dim                    # number of latent variables
-    nparams = vae.q_dist.nparams
-    vae.eval()
-
-    print('Computing q(z|x) distributions.')
-    qz_params = torch.Tensor(N, K, nparams)
-
-    n = 0
-    for xs in dataset_loader:
-        batch_size = xs.size(0)
-        xs = Variable(xs.view(batch_size, 1, 64, 64).cuda(), volatile=True)
-        qz_params[n:n + batch_size] = vae.encoder.forward(xs).view(batch_size, vae.z_dim, nparams).data
-        n += batch_size
-
-    qz_params = Variable(qz_params.view(3, 6, 40, 32, 32, K, nparams).cuda())
-    qz_samples = vae.q_dist.sample(params=qz_params)
-
-    print('Estimating marginal entropies.')
-    # marginal entropies
-    marginal_entropies = estimate_entropies(
-        qz_samples.view(N, K).transpose(0, 1),
-        qz_params.view(N, K, nparams),
-        vae.q_dist)
-
-    marginal_entropies = marginal_entropies.cpu()
-    cond_entropies = torch.zeros(4, K)
-
-    print('Estimating conditional entropies for scale.')
-    for i in range(6):
-        qz_samples_scale = qz_samples[:, i, :, :, :, :].contiguous()
-        qz_params_scale = qz_params[:, i, :, :, :, :].contiguous()
-
-        cond_entropies_i = estimate_entropies(
-            qz_samples_scale.view(N // 6, K).transpose(0, 1),
-            qz_params_scale.view(N // 6, K, nparams),
-            vae.q_dist)
-
-        cond_entropies[0] += cond_entropies_i.cpu() / 6
-
-    print('Estimating conditional entropies for orientation.')
-    for i in range(40):
-        qz_samples_scale = qz_samples[:, :, i, :, :, :].contiguous()
-        qz_params_scale = qz_params[:, :, i, :, :, :].contiguous()
-
-        cond_entropies_i = estimate_entropies(
-            qz_samples_scale.view(N // 40, K).transpose(0, 1),
-            qz_params_scale.view(N // 40, K, nparams),
-            vae.q_dist)
-
-        cond_entropies[1] += cond_entropies_i.cpu() / 40
-
-    print('Estimating conditional entropies for pos x.')
-    for i in range(32):
-        qz_samples_scale = qz_samples[:, :, :, i, :, :].contiguous()
-        qz_params_scale = qz_params[:, :, :, i, :, :].contiguous()
-
-        cond_entropies_i = estimate_entropies(
-            qz_samples_scale.view(N // 32, K).transpose(0, 1),
-            qz_params_scale.view(N // 32, K, nparams),
-            vae.q_dist)
-
-        cond_entropies[2] += cond_entropies_i.cpu() / 32
-
-    print('Estimating conditional entropies for pox y.')
-    for i in range(32):
-        qz_samples_scale = qz_samples[:, :, :, :, i, :].contiguous()
-        qz_params_scale = qz_params[:, :, :, :, i, :].contiguous()
-
-        cond_entropies_i = estimate_entropies(
-            qz_samples_scale.view(N // 32, K).transpose(0, 1),
-            qz_params_scale.view(N // 32, K, nparams),
-            vae.q_dist)
-
-        cond_entropies[3] += cond_entropies_i.cpu() / 32
-
-    metric = compute_metric_shapes(marginal_entropies, cond_entropies)
-    return metric, marginal_entropies, cond_entropies
-
-
-def mutual_info_metric_faces(vae, shapes_dataset):
-    dataset_loader = DataLoader(shapes_dataset, batch_size=1000, num_workers=1, shuffle=False)
-
-    N = len(dataset_loader.dataset)  # number of data samples
-    K = vae.z_dim                    # number of latent variables
-    nparams = vae.q_dist.nparams
-    vae.eval()
-
-    print('Computing q(z|x) distributions.')
-    qz_params = torch.Tensor(N, K, nparams)
-
-    n = 0
-    for xs in dataset_loader:
-        batch_size = xs.size(0)
-        xs = Variable(xs.view(batch_size, 1, 64, 64).cuda(), volatile=True)
-        qz_params[n:n + batch_size] = vae.encoder.forward(xs).view(batch_size, vae.z_dim, nparams).data
-        n += batch_size
-
-    qz_params = Variable(qz_params.view(50, 21, 11, 11, K, nparams).cuda())
-    qz_samples = vae.q_dist.sample(params=qz_params)
-
-    print('Estimating marginal entropies.')
-    # marginal entropies
-    marginal_entropies = estimate_entropies(
-        qz_samples.view(N, K).transpose(0, 1),
-        qz_params.view(N, K, nparams),
-        vae.q_dist)
-
-    marginal_entropies = marginal_entropies.cpu()
-    cond_entropies = torch.zeros(3, K)
-
-    print('Estimating conditional entropies for azimuth.')
-    for i in range(21):
-        qz_samples_pose_az = qz_samples[:, i, :, :, :].contiguous()
-        qz_params_pose_az = qz_params[:, i, :, :, :].contiguous()
-
-        cond_entropies_i = estimate_entropies(
-            qz_samples_pose_az.view(N // 21, K).transpose(0, 1),
-            qz_params_pose_az.view(N // 21, K, nparams),
-            vae.q_dist)
-
-        cond_entropies[0] += cond_entropies_i.cpu() / 21
-
-    print('Estimating conditional entropies for elevation.')
-    for i in range(11):
-        qz_samples_pose_el = qz_samples[:, :, i, :, :].contiguous()
-        qz_params_pose_el = qz_params[:, :, i, :, :].contiguous()
-
-        cond_entropies_i = estimate_entropies(
-            qz_samples_pose_el.view(N // 11, K).transpose(0, 1),
-            qz_params_pose_el.view(N // 11, K, nparams),
-            vae.q_dist)
-
-        cond_entropies[1] += cond_entropies_i.cpu() / 11
-
-    print('Estimating conditional entropies for lighting.')
-    for i in range(11):
-        qz_samples_lighting = qz_samples[:, :, :, i, :].contiguous()
-        qz_params_lighting = qz_params[:, :, :, i, :].contiguous()
-
-        cond_entropies_i = estimate_entropies(
-            qz_samples_lighting.view(N // 11, K).transpose(0, 1),
-            qz_params_lighting.view(N // 11, K, nparams),
-            vae.q_dist)
-
-        cond_entropies[2] += cond_entropies_i.cpu() / 11
-
-    metric = compute_metric_faces(marginal_entropies, cond_entropies)
-    return metric, marginal_entropies, cond_entropies
-
-
-if __name__ == '__main__':
-    import argparse
-    parser = argparse.ArgumentParser()
-    parser.add_argument('--checkpt', required=True)
-    parser.add_argument('--gpu', type=int, default=0)
-    parser.add_argument('--save', type=str, default='.')
-    args = parser.parse_args()
-
-    if args.gpu != 0:
-        torch.cuda.set_device(args.gpu)
-    vae, dataset, cpargs = load_model_and_dataset(args.checkpt)
-    metric, marginal_entropies, cond_entropies = eval('mutual_info_metric_' + cpargs.dataset)(vae, dataset)
-    torch.save({
-        'metric': metric,
-        'marginal_entropies': marginal_entropies,
-        'cond_entropies': cond_entropies,
-    }, os.path.join(args.save, 'disentanglement_metric.pth'))
-    print('MIG: {:.2f}'.format(metric))
diff --git a/elbo_decomposition.py b/elbo_decomposition.py
deleted file mode 100644
index 59ceb83..0000000
--- a/elbo_decomposition.py
+++ /dev/null
@@ -1,252 +0,0 @@
-import os
-import math
-from numbers import Number
-from tqdm import tqdm
-import torch
-from torch.autograd import Variable
-
-import lib.dist as dist
-import lib.flows as flows
-
-
-def estimate_entropies(qz_samples, qz_params, q_dist, use_cuda):
-    """Computes the term:
-        E_{p(x)} E_{q(z|x)} [-log q(z)]
-    and
-        E_{p(x)} E_{q(z_j|x)} [-log q(z_j)]
-    where q(z) = 1/N sum_n=1^N q(z|x_n).
-    Assumes samples are from q(z|x) for *all* x in the dataset.
-    Assumes that q(z|x) is factorial ie. q(z|x) = prod_j q(z_j|x).
-
-    Computes numerically stable NLL:
-        - log q(z) = log N - logsumexp_n=1^N log q(z|x_n)
-
-    Inputs:
-    -------
-        qz_samples (K, S) Variable
-        qz_params  (N, K, nparams) Variable
-    """
-
-    # Only take a sample subset of the samples
-    if use_cuda:
-        qz_samples = qz_samples.cuda()
-    qz_samples = qz_samples.index_select(1, Variable(torch.randperm(qz_samples.size(1))[:10000]))
-
-    K, S = qz_samples.size()
-    N, _, nparams = qz_params.size()
-    assert(nparams == q_dist.nparams)
-    assert(K == qz_params.size(1))
-
-    marginal_entropies = torch.zeros(K)
-    joint_entropy = torch.zeros(1)
-
-    if use_cuda:
-        marginal_entropies = marginal_entropies.cuda()
-        joint_entropy = joint_entropy.cuda()
-
-
-    pbar = tqdm(total=S)
-    k = 0
-    while k < S:
-        batch_size = min(10, S - k)
-        logqz_i = q_dist.log_density(
-            qz_samples.view(1, K, S).expand(N, K, S)[:, :, k:k + batch_size],
-            qz_params.view(N, K, 1, nparams).expand(N, K, S, nparams)[:, :, k:k + batch_size])
-        k += batch_size
-
-        # computes - log q(z_i) summed over minibatch
-        marginal_entropies += (math.log(N) - logsumexp(logqz_i, dim=0, keepdim=False).data).sum(1)
-        # computes - log q(z) summed over minibatch
-        logqz = logqz_i.sum(1)  # (N, S)
-        joint_entropy += (math.log(N) - logsumexp(logqz, dim=0, keepdim=False).data).sum(0)
-        pbar.update(batch_size)
-    pbar.close()
-
-    marginal_entropies /= S
-    joint_entropy /= S
-
-    return marginal_entropies, joint_entropy
-
-
-def logsumexp(value, dim=None, keepdim=False):
-    """Numerically stable implementation of the operation
-
-    value.exp().sum(dim, keepdim).log()
-    """
-    if dim is not None:
-        m, _ = torch.max(value, dim=dim, keepdim=True)
-        value0 = value - m
-        if keepdim is False:
-            m = m.squeeze(dim)
-        return m + torch.log(torch.sum(torch.exp(value0),
-                                       dim=dim, keepdim=keepdim))
-    else:
-        m = torch.max(value)
-        sum_exp = torch.sum(torch.exp(value - m))
-        if isinstance(sum_exp, Number):
-            return m + math.log(sum_exp)
-        else:
-            return m + torch.log(sum_exp)
-
-
-def analytical_NLL(qz_params, q_dist, prior_dist, qz_samples=None):
-    """Computes the quantities
-        1/N sum_n=1^N E_{q(z|x)} [ - log q(z|x) ]
-    and
-        1/N sum_n=1^N E_{q(z_j|x)} [ - log p(z_j) ]
-
-    Inputs:
-    -------
-        qz_params  (N, K, nparams) Variable
-
-    Returns:
-    --------
-        nlogqz_condx (K,) Variable
-        nlogpz (K,) Variable
-    """
-    pz_params = Variable(torch.zeros(1).type_as(qz_params.data).expand(qz_params.size()), volatile=True)
-
-    nlogqz_condx = q_dist.NLL(qz_params).mean(0)
-    nlogpz = prior_dist.NLL(pz_params, qz_params).mean(0)
-    return nlogqz_condx, nlogpz
-
-
-def elbo_decomposition(vae, dataset_loader, use_cuda):
-    N = len(dataset_loader.dataset)  # number of data samples
-    K = vae.z_dim                    # number of latent variables
-    S = 1                            # number of latent variable samples
-    nparams = vae.q_dist.nparams
-
-    print('Computing q(z|x) distributions.')
-    # compute the marginal q(z_j|x_n) distributions
-    qz_params = torch.Tensor(N, K, nparams)
-    n = 0
-    logpx = 0
-
-    for xs,y in dataset_loader:
-        print(type(xs))
-        print(xs.shape)
-        batch_size = xs.size(0)
-
-
-    # for i, x in enumerate(dataset_loader):
-    #     x = x[0] #only for mnist
-    #     batch_size = xs.size(0)
-        xs = Variable(xs.view(batch_size, -1, 28, 28), volatile=True)
-        z_params = vae.encoder.forward(xs).view(batch_size, K, nparams)
-        qz_params[n:n + batch_size] = z_params.data
-        n += batch_size
-
-        # estimate reconstruction term
-        for _ in range(S):
-            z = vae.q_dist.sample(params=z_params)
-            x_params = vae.decoder.forward(z)
-            logpx += vae.x_dist.log_density(xs, params=x_params).view(batch_size, -1).data.sum()
-    # Reconstruction term
-    logpx = logpx / (N * S)
-
-    if use_cuda:
-        qz_params = qz_params.cuda()
-    qz_params = Variable(qz_params, volatile=True)
-
-
-    print('Sampling from q(z).')
-    # sample S times from each marginal q(z_j|x_n)
-    qz_params_expanded = qz_params.view(N, K, 1, nparams).expand(N, K, S, nparams)
-    qz_samples = vae.q_dist.sample(params=qz_params_expanded)
-    qz_samples = qz_samples.transpose(0, 1).contiguous().view(K, N * S)
-
-    print('Estimating entropies.')
-    marginal_entropies, joint_entropy = estimate_entropies(qz_samples, qz_params, vae.q_dist)
-
-    if hasattr(vae.q_dist, 'NLL'):
-        nlogqz_condx = vae.q_dist.NLL(qz_params).mean(0)
-    else:
-        nlogqz_condx = - vae.q_dist.log_density(qz_samples,
-            qz_params_expanded.transpose(0, 1).contiguous().view(K, N * S)).mean(1)
-
-    if hasattr(vae.prior_dist, 'NLL'):
-        pz_params = vae._get_prior_params(N * K).contiguous().view(N, K, -1)
-        nlogpz = vae.prior_dist.NLL(pz_params, qz_params).mean(0)
-    else:
-        nlogpz = - vae.prior_dist.log_density(qz_samples.transpose(0, 1)).mean(0)
-
-    # nlogqz_condx, nlogpz = analytical_NLL(qz_params, vae.q_dist, vae.prior_dist)
-    nlogqz_condx = nlogqz_condx.data
-    nlogpz = nlogpz.data
-
-    # Independence term
-    # KL(q(z)||prod_j q(z_j)) = log q(z) - sum_j log q(z_j)
-    dependence = (- joint_entropy + marginal_entropies.sum())[0]
-
-    # Information term
-    # KL(q(z|x)||q(z)) = log q(z|x) - log q(z)
-    information = (- nlogqz_condx.sum() + joint_entropy)[0]
-
-    # Dimension-wise KL term
-    # sum_j KL(q(z_j)||p(z_j)) = sum_j (log q(z_j) - log p(z_j))
-    dimwise_kl = (- marginal_entropies + nlogpz).sum()
-
-    # Compute sum of terms analytically
-    # KL(q(z|x)||p(z)) = log q(z|x) - log p(z)
-    analytical_cond_kl = (- nlogqz_condx + nlogpz).sum()
-
-    print('Dependence: {}'.format(dependence))
-    print('Information: {}'.format(information))
-    print('Dimension-wise KL: {}'.format(dimwise_kl))
-    print('Analytical E_p(x)[ KL(q(z|x)||p(z)) ]: {}'.format(analytical_cond_kl))
-    print('Estimated  ELBO: {}'.format(logpx - analytical_cond_kl))
-
-    return logpx, dependence, information, dimwise_kl, analytical_cond_kl, marginal_entropies, joint_entropy
-
-
-if __name__ == '__main__':
-    import argparse
-    parser = argparse.ArgumentParser()
-    parser.add_argument('-checkpt', required=True)
-    parser.add_argument('-save', type=str, default='.')
-    parser.add_argument('-gpu', type=int, default=0)
-    args = parser.parse_args()
-
-    def load_model_and_dataset(checkpt_filename):
-        checkpt = torch.load(checkpt_filename)
-        args = checkpt['args']
-        state_dict = checkpt['state_dict']
-
-        # backwards compatibility
-        if not hasattr(args, 'conv'):
-            args.conv = False
-
-        from vae_quant import VAE, setup_data_loaders
-
-        # model
-        if args.dist == 'normal':
-            prior_dist = dist.Normal()
-            q_dist = dist.Normal()
-        elif args.dist == 'laplace':
-            prior_dist = dist.Laplace()
-            q_dist = dist.Laplace()
-        elif args.dist == 'flow':
-            prior_dist = flows.FactorialNormalizingFlow(dim=args.latent_dim, nsteps=32)
-            q_dist = dist.Normal()
-        vae = VAE(z_dim=args.latent_dim, use_cuda=True, prior_dist=prior_dist, q_dist=q_dist, conv=args.conv)
-        vae.load_state_dict(state_dict, strict=False)
-        vae.eval()
-
-        # dataset loader
-        loader = setup_data_loaders(args, use_cuda=True)
-        return vae, loader
-
-    torch.cuda.set_device(args.gpu)
-    vae, dataset_loader = load_model_and_dataset(args.checkpt)
-    logpx, dependence, information, dimwise_kl, analytical_cond_kl, marginal_entropies, joint_entropy = \
-        elbo_decomposition(vae, dataset_loader)
-    torch.save({
-        'logpx': logpx,
-        'dependence': dependence,
-        'information': information,
-        'dimwise_kl': dimwise_kl,
-        'analytical_cond_kl': analytical_cond_kl,
-        'marginal_entropies': marginal_entropies,
-        'joint_entropy': joint_entropy
-    }, os.path.join(args.save, 'elbo_decomposition.pth'))
diff --git a/lib/utils.py b/lib/utils.py
index 112a1fe..a022e7c 100644
--- a/lib/utils.py
+++ b/lib/utils.py
@@ -2,6 +2,7 @@ from numbers import Number
 import math
 import torch
 import os
+from torch.utils.data import DataLoader
 
 
 def save_checkpoint(state, save, epoch):
@@ -71,3 +72,48 @@ def logsumexp(value, dim=None, keepdim=False):
             return m + math.log(sum_exp)
         else:
             return m + torch.log(sum_exp)
+
+
+# for loading and batching datasets
+def setup_data_loaders(args, use_cuda=False):
+    from torchvision.transforms import ToTensor, Lambda
+    from torchvision import datasets
+
+
+    if args.dataset == 'shapes':
+        train_set = dset.Shapes()
+    elif args.dataset == 'faces':
+        train_set = dset.Faces()
+    elif args.dataset == 'mnist':
+        train_set = datasets.MNIST(
+        root="data",
+        train=True,
+        download=True,
+        transform=ToTensor())
+
+    else:
+        raise ValueError('Unknown dataset ' + str(args.dataset))
+
+    kwargs = {'num_workers': 0, 'pin_memory': use_cuda}
+
+
+    train_loader = DataLoader(dataset=train_set,
+        batch_size=args.batch_size, shuffle=True, **kwargs)
+    return train_loader
+
+
+
+def anneal_kl(args, vae, iteration):
+    if args.dataset == 'shapes':
+        warmup_iter = 7000
+    elif args.dataset == 'faces':
+        warmup_iter = 2500
+
+    if args.lambda_anneal:
+        vae.lamb = max(0, 0.95 - 1 / warmup_iter * iteration)  # 1 --> 0
+    else:
+        vae.lamb = 0
+    if args.beta_anneal:
+        vae.beta = min(args.beta, args.beta / warmup_iter * iteration)  # 0 --> 1
+    else:
+        vae.beta = args.beta
diff --git a/plot_latent_vs_true.py b/plot_latent_vs_true.py
index 61d1cc7..c18495b 100644
--- a/plot_latent_vs_true.py
+++ b/plot_latent_vs_true.py
@@ -189,6 +189,98 @@ def plot_vs_gt_faces(vae, faces_dataset, save, z_inds=None):
     plt.savefig(save)
     plt.close()
 
+def plot_vs_gt_mnist(vae, mnist_dataset, save, z_inds=None, use_cuda = False):
+    dataset_loader = DataLoader(mnist_dataset, batch_size=32, num_workers=1, shuffle=False)
+
+    N = len(dataset_loader.dataset)  # number of data samples
+    K = vae.z_dim                    # number of latent variables
+    nparams = vae.q_dist.nparams
+    vae.eval()
+
+    # print('Computing q(z|x) distributions.')
+    qz_params = torch.Tensor(N, K, nparams)
+
+    n = 0
+    for i, (xs,y) in enumerate(dataset_loader):
+        batch_size = xs.size(0)
+        xs = Variable(xs.view(batch_size, 1, 28, 28), volatile=True)
+        if use_cuda:
+            xs = xs.cuda()
+        qz_params[n:n + batch_size] = vae.encoder.forward(xs).view(batch_size, vae.z_dim, nparams).data
+        n += batch_size
+
+    qz_params = qz_params.view(3, 10, 5, 20, 20, K, nparams)
+
+    # z_j is inactive if Var_x(E[z_j|x]) < eps.
+    qz_means = qz_params[:, :, :, :, :, :, 0]
+    var = torch.std(qz_means.contiguous().view(N, K), dim=0).pow(2)
+    active_units = torch.arange(0, K)[var > VAR_THRESHOLD].long()
+    print('Active units: ' + ','.join(map(str, active_units.tolist())))
+    n_active = len(active_units)
+    print('Number of active units: {}/{}'.format(n_active, vae.z_dim))
+
+    if z_inds is None:
+        z_inds = active_units
+
+    # subplots where subplot[i, j] is gt_i vs. z_j
+    mean_scale = qz_means.mean(2).mean(2).mean(2)  # (shape, scale, latent)
+    mean_rotation = qz_means.mean(1).mean(2).mean(2)  # (shape, rotation, latent)
+    mean_pos = qz_means.mean(0).mean(0).mean(0)  # (pos_x, pos_y, latent)
+
+    fig = plt.figure(figsize=(3, len(z_inds)))  # default is (8,6)
+    gs = gridspec.GridSpec(len(z_inds), 3)
+    gs.update(wspace=0, hspace=0)  # set the spacing between axes.
+
+    vmin_pos = torch.min(mean_pos)
+    vmax_pos = torch.max(mean_pos)
+    for i, j in enumerate(z_inds):
+        ax = fig.add_subplot(gs[i * 3])
+        ax.imshow(mean_pos[:, :, j].numpy(), cmap=plt.get_cmap('coolwarm'), vmin=vmin_pos, vmax=vmax_pos)
+        ax.set_xticks([])
+        ax.set_yticks([])
+        ax.set_ylabel(r'$z_' + str(j) + r'$')
+        if i == len(z_inds) - 1:
+            ax.set_xlabel(r'pos')
+
+    vmin_scale = torch.min(mean_scale)
+    vmax_scale = torch.max(mean_scale)
+    for i, j in enumerate(z_inds):
+        ax = fig.add_subplot(gs[1 + i * 3])
+        ax.plot(mean_scale[0, :, j].numpy(), color=colors[2])
+        ax.plot(mean_scale[1, :, j].numpy(), color=colors[0])
+        ax.plot(mean_scale[2, :, j].numpy(), color=colors[1])
+        ax.set_ylim([vmin_scale, vmax_scale])
+        ax.set_xticks([])
+        ax.set_yticks([])
+        x0, x1 = ax.get_xlim()
+        y0, y1 = ax.get_ylim()
+        ax.set_aspect(abs(x1 - x0) / abs(y1 - y0))
+        if i == len(z_inds) - 1:
+            ax.set_xlabel(r'scale')
+
+    vmin_rotation = torch.min(mean_rotation)
+    vmax_rotation = torch.max(mean_rotation)
+    for i, j in enumerate(z_inds):
+        ax = fig.add_subplot(gs[2 + i * 3])
+        ax.plot(mean_rotation[0, :, j].numpy(), color=colors[2])
+        ax.plot(mean_rotation[1, :, j].numpy(), color=colors[0])
+        ax.plot(mean_rotation[2, :, j].numpy(), color=colors[1])
+        ax.set_ylim([vmin_rotation, vmax_rotation])
+        ax.set_xticks([])
+        ax.set_yticks([])
+        x0, x1 = ax.get_xlim()
+        y0, y1 = ax.get_ylim()
+        ax.set_aspect(abs(x1 - x0) / abs(y1 - y0))
+        if i == len(z_inds) - 1:
+            ax.set_xlabel(r'rotation')
+
+    fig.text(0.5, 0.03, 'Ground Truth', ha='center')
+    fig.text(0.01, 0.5, 'Learned Latent Variables ', va='center', rotation='vertical')
+    plt.savefig(save)
+    plt.close()
+
+
+
 
 if __name__ == '__main__':
     import argparse
diff --git a/test/checkpt-0000.pth b/test/checkpt-0000.pth
index 296d2a7..b6b5639 100644
Binary files a/test/checkpt-0000.pth and b/test/checkpt-0000.pth differ
